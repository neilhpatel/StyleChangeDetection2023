I found some very easy and robust ways to get it to demonstrate how not smart it was. Basically, it can't lie because the bot proceeds with all inputs as if they are related to the material it trained on. So if you ask it to do something over false premises it fails immediately. It can detect nonsense and gibberish, but it can't detect that something was just made up.
It will say something like "The [] conjecture does not apply to []'s postulate. []'s postulate is best understood as the blah blah blah blah. []'s conjecture has been shown to be true when blah blah blah blah.".
It’s more than that. In those regimes, an admission of fucking up can be a death sentence. So no one is going to admit wrong doing - they will look to place blame. Everyone knows everyone else is lying - but if the guy below you fucked up, you’re also responsible. It creates a toxic culture where mistakes can’t be studied and corrected and is why those regimes often have piss poor militaries and piss poor government administrators. It’s more important to play the political game than to be competent.